---
title: "Step 1: Retrieve and Clean Seed Set"
output: html_notebook
---

# Overview {.tabset}

This portion consists of 4 steps:

1.    Retrieve initial records from query
2.    Retrieve abstracts
3.    Extract and clean URLs from abstracts
4.    Find string matches between title and URLs

```{r setup, include=FALSE}
library(dplyr)
library(europepmc)
library(stringr)
library(tidyr)
```

## 1. Retrieval

The following query string is used in retrieving the initial seed set.

```{r query, echo = FALSE}
my_query <-
  '(((ABSTRACT:"www" OR ABSTRACT:"http" OR ABSTRACT:"https") AND (ABSTRACT:"data" OR ABSTRACT:"resource" OR ABSTRACT:"database"))  NOT (TITLE:"retraction" OR TITLE:"retracted" OR TITLE:"withdrawn" OR TITLE:"withdrawal" OR TITLE:"erratum") NOT ((ABSTRACT:"retract" OR ABSTRACT:"withdraw" ABSTRACT:"erratum" OR ABSTRACT:"github.com" OR ABSTRACT:"github.io" OR ABSTRACT:"cran.r" OR ABSTRACT:"youtube.com" OR ABSTRACT:"bitbucket.org" OR ABSTRACT:"links.lww.com" OR ABSTRACT:"osf.io" OR ABSTRACT:"bioconductor.org" OR ABSTRACT:"annualreviews.org" OR ABSTRACT:"creativecommons.org" OR ABSTRACT:"sourceforge.net" OR ABSTRACT:".pdf" OR ABSTRACT:"clinical trial" OR ABSTRACT:"registry" OR ABSTRACT:"registration" OR ABSTRACT:"trial registration" OR ABSTRACT:"clinicaltrial" OR ABSTRACT:"registration number" OR ABSTRACT:"pre-registration" OR ABSTRACT:"preregistration"))) AND (((SRC:MED OR SRC:PMC OR SRC:AGR OR SRC:CBA))) AND (FIRST_PDATE:[2011 TO 2021])'

my_query
```

```{r retrieve, include=FALSE}
pmc_seed <- epmc_search(query = my_query, limit = 25000, verbose = FALSE)

rm(my_query)
```

The seed set is filtered to exclude an `NA` rows and publications before 2010.

```{r initial_measures, echo = FALSE}
print(str_glue("Total number of records: {nrow(pmc_seed)}"))

pmc_seed_complete_cases <- filter(pmc_seed,!is.na(pmc_seed$id))
print(str_glue("Number of records: {nrow(pmc_seed_complete_cases)}"))

pmc_seed_filtered <- filter(pmc_seed, pubYear > 2010)
print(str_glue("Number of records after 2010: {nrow(pmc_seed_filtered)}"))

count(pmc_seed, journalTitle)
```

```{r write_seed, echo=FALSE}
out_file <- "pmc_seed_all.csv"

out_path <- file.path("..", "data", "raw", out_file)

rm(out_file)

write.csv(pmc_seed_filtered, out_path, row.names = FALSE)

print(str_glue("Initial seed saved to {out_path}."))
```

## 2. Retrieve abstracts

First, read in the seed set and manually checked file.
```{r read_seed, echo=FALSE}
pmc_seed <- read.csv("../data/raw/pmc_seed_all_2021-08-06.csv")

manual_checks <- read.csv("../data/processed/manual_checks_hji_2021-08-20.csv")
```

```{r abstracts, echo = FALSE}
abstracts <- pmc_seed %>%
  select(id, title) %>% 
  mutate(abstract = NA)
```

### Get Abstracts from Default Source

Get the abstracts of the papers. Generally, I don't use for loops in R since R is optimized for vector operations. However, the details object returned is quite large. So by only storing 1 in memory at a time, the memory load is greatly reduced. The trade-off is computation time is increased.

```{r get_abstracts, message=FALSE}
for (id in abstracts$id) {
  
  details <- epmc_details(id)
  
  if (!is.null(details)) {
    if ("basic" %in% names(details)) {
      if ("abstractText" %in% colnames(details$basic)) {
        abstract <- details$basic$abstractText
        abstracts$abstract[abstracts$id == id] <- abstract
      }
    }
  }
}

rm(details, abstract, id)
```

For which papers were we not able to retrieve abstracts?

*Note:* Abstract retrieval so far was only done using the default source (`"med"`).

```{r, echo=FALSE}
no_abstract <- abstracts %>% 
  filter(is.na(abstract)) %>%
  right_join(pmc_seed %>% select(id, source), ., by="id")

num_missing <- count(no_abstract)
num_total <- count(abstracts)
prop_missing <- round((num_missing / num_total)*100, digits = 2)

print(str_glue("Papers with missing abstract: {count(no_abstract)}"), 
      str_glue("Proportion missing abstract: {prop_missing}%"))
no_abstract %>% count(source)

rm(num_missing, num_total, prop_missing)
```

### Grab other abstracts

This step will try to get the remaining abstracts by querying all sources. This step can also be used if more papers are added later, without retrieving all abstracts again. 

This would be a good step to include in the pipeline. *e.g.* pass an abstracts file optionally to this step of the script. If no abstracts, retrieve everything, otherwise just retrieve the missing ones. The list of sources could also be automated by looking at which sources are in the list of papers with missing abstracts.

```{r get missing, message=FALSE}
for (id in no_abstract$id) {
  for (data_source in c("agr", "pmc", "med")) {
    details <- epmc_details(id, data_src = data_source)
    
    if (!is.null(details)) {
      if ("basic" %in% names(details)) {
        if ("abstractText" %in% colnames(details$basic)) {
          abstract <- details$basic$abstractText
          abstracts$abstract[abstracts$id == id] <- abstract
          break
        }
      }
    }
  }
}

rm(data_source, details, abstract, id)
```


```{r, echo=FALSE}
no_abstract <- abstracts %>% 
  filter(is.na(abstract)) %>%
  right_join(pmc_seed %>% select(id, source), ., by="id")

num_missing <- count(no_abstract)
num_total <- count(abstracts)
prop_missing <- round((num_missing / num_total)*100, digits = 2)

print(str_glue("Papers still missing abstract: {count(no_abstract)}"), 
      str_glue("Proportion missing abstract: {prop_missing}%"))
no_abstract %>% count(source)

rm(num_missing, num_total, prop_missing, no_abstract)
```

Save the abstracts to a file so they don't have to be retrieved again.

```{r save_abstracts, echo=FALSE}
out_file <- "abstracts.csv"

out_path <- file.path("..", "data", "raw", out_file)

rm(out_file)

write.csv(abstracts, out_path, row.names = FALSE)

print(str_glue("Abstracts written to: {out_path}"))
```

## 3. URL Cleaning

### Read in the abstracts

Read in the abstracts that have been already fetched.

```{r read_abstracts, echo = FALSE}
abstracts <- read.csv("../data/raw/abstracts.csv")
```

### Initial Capture

The following regular expression is used to describe a URL

```{r url_regex, echo = FALSE}
url_pattern <- "http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+"

url_pattern
```

Extracting the raw matches, and unnesting for cases in which multiple URL's were found in an abstract.
```{r extract_urls}
dev_urls <- abstracts

dev_urls$url_match <- dev_urls$abstract %>% 
  str_extract_all(url_pattern)

dev_urls <- dev_urls %>% 
  unnest(cols = url_match) %>% 
  filter(!is.na(url_match))

dev_urls %>% select(url_match)
```

### Cleaning Up

Trimming html headers from the end of strings.

Literally removing "<" followed by 0 or more of any character to end of string.

```{r remove_html_tag}
dev_urls$clean_url <- dev_urls$url_match %>%
  str_replace("<.*$", "")

dev_urls %>% select(clean_url)
```

Trimming 1 or more of anything in the character class `[.)-;:?*&>]` and also literal "]".

*I added the `+` so that not just the last character in that class is trimmed*

*I also added the square bracket to the character class*

```{r remove_ends}
dev_urls$clean_url <- dev_urls$clean_url %>%
  str_replace("[.)-;:?*&>\\]]+$", "")

dev_urls %>% select(clean_url)
```

Removing 1 or more whitespace characters

```{r remove whitespace}
dev_urls$clean_url <- dev_urls$clean_url %>%
  str_replace("\\s+", "")

dev_urls %>% select(clean_url)
```


### Remove URLs from Specific Sources

Remove URL's that have certain source names in them (case insensitive).

We may want to add:

* doi.org
* gitlab

```{r remove_url_sources}
false_sources <- c(
  "clinicaltrials",
  "annualreviews",
  "onlinelibrary.wiley.com",
  "cos.io",
  "elsevier.com"
)

combined_sources <- paste0("(?i)", paste(false_sources, collapse = "|"))

dev_urls <- dev_urls %>% 
  filter(!str_detect(clean_url, combined_sources))

rm(false_sources, combined_sources)

dev_urls %>% select(clean_url)
```

```{r save_urls, echo=FALSE}
urls <- dev_urls %>% 
  select(-abstract, -url_match)

out_file <- "urls.csv"

out_path <- file.path("..", "data", "processed", out_file)

rm(out_file)

write.csv(urls, out_path, row.names = FALSE)

print(str_glue("URLs written to: {out_path}"))
```


## 4. URL Title Matching

In this section, we will try to find matching strings between the title and the URL found in the abstract.

```{r read_urls, echo=FALSE}
urls <- read.csv("../data/processed/urls.csv")
```

First split the title into a list on whitespace, and only take unique elements

```{r split_title}
urls <- urls %>% 
  rename(url = clean_url) %>% 
  mutate(title_url_match = NA)

urls$title_words <- urls$title %>%
  str_replace_all("[:punct:]", " ") %>%
  tolower() %>%
  str_split(" +") %>% 
  lapply(unique)

```

Next, split the URLs on punctuation, but first remove things like:

* `https://`, `http://`, `https://www.`, `http://www.`
* endings of `.com`, `.org`, and `.edu`

```{r split_url}
urls$url_words <- urls$url %>%
  str_remove_all("^http(?:s://(?:www\\.)?|://(?:www\\.)?)") %>% 
  str_remove_all("\\.(?:com|edu|org)$") %>% 
  str_replace_all("[:punct:]", " ") %>%
  tolower() %>%
  str_split(" +") %>% 
  lapply(unique)
```

Then find matching words from the split titles and URLs. Remove duplicate matches per ID.

```{r find_matches}
urls$title_url_match <- mapply(intersect, urls$title_words, urls$url_words)

urls <- urls %>% 
  unnest(cols = title_url_match) %>% 
  distinct(title_url_match, id, .keep_all = TRUE) %>% 
  select(-title_words, -url_words)

urls %>% select(title_url_match)
```

